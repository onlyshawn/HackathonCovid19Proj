{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovOmsEWCwOYl",
        "outputId": "937fb1be-55c2-4ca0-95e9-26c450ed4e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/MyDrive/\"\n",
        "base_dir = root_dir + 'projects/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GLlQD_RyBNc"
      },
      "outputs": [],
      "source": [
        "!pip install tensorlayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlsegoNcwibD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0332ce-d2b2-4bc6-aeb5-116c0a6be668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/projects/HackathonCovid19Proj\n",
            "Loading data...\n",
            "CT-0\n",
            "Total number of images is 11134\n",
            "There are 1 classes in total:\n",
            "['CT-0']\n",
            "Corresponding data sample number for each class:\n",
            "[11134]\n",
            "Image size:\n",
            "(512, 512)\n",
            "[11134]\n",
            "Shape for data samples in class CT-0\n",
            "(11134, 64, 64)\n",
            "(11134, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============Statistical Information===========\n",
            "Training Set: \n",
            "Shape of data: (10688, 1, 64, 64); Shape of labels: (10688,)\n",
            "Test Set: \n",
            "Shape of data: (446, 1, 64, 64); Shape of labels: (446,)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/MyDrive/projects/HackathonCovid19Proj\")\n",
        "!pwd\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy.random as random\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "from utils import *\n",
        "from DataLoader import *\n",
        "from Losses import *\n",
        "import argparse\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "from model.ResNet import resnet18\n",
        "\n",
        "\n",
        "dic = {'CT-0': 0}\n",
        "rev_dic = {0 : 'CT-0'}\n",
        "metric_basic = ['acc']\n",
        "metric_ad = ['MCC']\n",
        "avg = ['micro', 'macro']\n",
        "classes = len(dic)\n",
        "\n",
        "def createTrainTestData(test_split=0.04, val_split=0.20, seed = 1997\n",
        "              ):\n",
        "    # Initialize\n",
        "    if (not os.path.exists(\"./cache\")):\n",
        "        image_data, labels, cls_num_list = loadRawData()\n",
        "        print(cls_num_list)\n",
        "        image_data_per_class = [[] for i in range(len(image_data))]\n",
        "        for i in range(classes):\n",
        "            \n",
        "            image_data_per_class[i] = concatenate(image_data[i])\n",
        "            print(\"Shape for data samples in class \" + rev_dic[i])\n",
        "            print(image_data_per_class[i].shape)\n",
        "            print(labels[i].shape)\n",
        "            np.random.shuffle(image_data_per_class[i])\n",
        "\n",
        "        # Split training set, validation set and test set\n",
        "        X_train = image_data_per_class[0][:int(image_data_per_class[0].shape[0] * (1 - test_split))]\n",
        "        Y_train = labels[0][:int(image_data_per_class[0].shape[0] * (1 - test_split))]\n",
        "                          \n",
        "        X_test = np.vstack( image_data_per_class[i][int(image_data_per_class[i].shape[0] * (1 - test_split)):]\n",
        "                            for i in range(classes)\n",
        "                             )\n",
        "        Y_test= np.vstack( labels[i][int(image_data_per_class[i].shape[0] * (1 - test_split)):]\n",
        "                            for i in range(classes)\n",
        "                             )\n",
        "        cls_num_list_np = np.array(cls_num_list)\n",
        "        os.makedirs(\"./cache\")\n",
        "        np.save(\"./cache/X_train.npy\", X_train)\n",
        "        np.save(\"./cache/Y_train.npy\", Y_train)\n",
        "        #np.save(\"./cache64/X_val.npy\", X_val)\n",
        "        #np.save(\"./cache64/Y_val.npy\", Y_val)\n",
        "        np.save(\"./cache/X_test.npy\", X_test)\n",
        "        np.save(\"./cache/Y_test.npy\", Y_test)\n",
        "        np.save(\"./cache/cls.npy\", cls_num_list_np)\n",
        "\n",
        "    else:\n",
        "        X_train = np.load(\"./cache/X_train.npy\")\n",
        "        Y_train = np.load(\"./cache/Y_train.npy\")\n",
        "        #X_val = np.load(\"./cache64/X_val.npy\")\n",
        "        #Y_val = np.load(\"./cache64/Y_val.npy\")\n",
        "        X_test = np.load(\"./cache/X_test.npy\")\n",
        "        Y_test = np.load(\"./cache/Y_test.npy\")\n",
        "        cls_num_list_np = np.load(\"./cache/cls.npy\")\n",
        "        cls_num_list = cls_num_list_np.tolist()\n",
        "\n",
        "    state = np.random.get_state()\n",
        "    np.random.shuffle(X_train)\n",
        "    np.random.set_state(state)\n",
        "    np.random.shuffle(Y_train)\n",
        "\n",
        "    X_train = X_train[:, np.newaxis].astype(np.float32);\n",
        "    #X_val = X_val[:, np.newaxis].astype(np.float32);\n",
        "    X_test = X_test[:, np.newaxis].astype(np.float32);\n",
        "    Y_train = np.squeeze(Y_train.astype(np.int64))\n",
        "    #Y_val = np.squeeze(Y_val.astype(np.int64))\n",
        "    Y_test = np.squeeze(Y_test.astype(np.int64))\n",
        "    print(\"============Statistical Information===========\")\n",
        "    print(\"Training Set: \")\n",
        "    print(\"Shape of data: \" + str(X_train.shape) + \"; Shape of labels: \" + str(Y_train.shape))\n",
        "    #print(\"Validation Set: \")\n",
        "    #print(\"Shape of data: \" + str(X_val.shape) + \"; Shape of labels: \" + str(Y_val.shape))\n",
        "    print(\"Test Set: \")\n",
        "    print(\"Shape of data: \" + str(X_test.shape) + \"; Shape of labels: \" + str(Y_test.shape))\n",
        "    # shuffle\n",
        "\n",
        "    #return (X_train, Y_train), (X_val, Y_val), (X_test, Y_test), cls_num_list[:classes]\n",
        "    return (X_train, Y_train), (X_test, Y_test), cls_num_list[:classes]\n",
        "\n",
        "\n",
        "# load data\n",
        "print('Loading data...')\n",
        "#train_set, val_set, test_set, cls_num_list = createTrainTestData()\n",
        "train_set, test_set, cls_num_list = createTrainTestData()\n",
        "X_train, Y_train = train_set\n",
        "#X_val, Y_val = val_set\n",
        "X_test, Y_test = test_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieXzQ14zFpz-",
        "outputId": "371b0bb2-00ce-4771-fceb-c81e0f1ec7cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/projects/HackathonCovid19Proj\n",
            "============Statistical Information===========\n",
            "Training Set: \n",
            "Shape of X_train0: (2448, 1, 64, 64); Shape of labels: (2448,)\n",
            "Shape of X_train1: (6566, 1, 64, 64); Shape of labels: (6566,)\n",
            "Shape of X_train2: (1200, 1, 64, 64); Shape of labels: (1200,)\n",
            "Shape of X_train3: (432, 1, 64, 64); Shape of labels: (432,)\n",
            "Test Set: \n",
            "Shape of X_test0: (102, 1, 64, 64); Shape of labels: (102,)\n",
            "Shape of X_test1: (274, 1, 64, 64); Shape of labels: (274,)\n",
            "Shape of X_test2: (50, 1, 64, 64); Shape of labels: (50,)\n",
            "Shape of X_test3: (18, 1, 64, 64); Shape of labels: (18,)\n"
          ]
        }
      ],
      "source": [
        "#read data from cache\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/MyDrive/projects/HackathonCovid19Proj\")\n",
        "!pwd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X_train0 = np.load(\"./ct0/X_train.npy\")\n",
        "Y_train0 = np.load(\"./ct0/Y_train.npy\")\n",
        "X_test0 = np.load(\"./ct0/X_test.npy\")\n",
        "Y_test0 = np.load(\"./ct0/Y_test.npy\")\n",
        "X_train0 = X_train0[:, np.newaxis].astype(np.float32);\n",
        "X_test0 = X_test0[:, np.newaxis].astype(np.float32);\n",
        "Y_train0 = np.squeeze(Y_train0.astype(np.int64))\n",
        "Y_test0 = np.squeeze(Y_test0.astype(np.int64))\n",
        "\n",
        "X_train1 = np.load(\"./ct1/X_train.npy\")\n",
        "Y_train1 = np.load(\"./ct1/Y_train.npy\")\n",
        "X_test1 = np.load(\"./ct1/X_test.npy\")\n",
        "Y_test1 = np.load(\"./ct1/Y_test.npy\")\n",
        "X_train1 = X_train1[:, np.newaxis].astype(np.float32);\n",
        "X_test1 = X_test1[:, np.newaxis].astype(np.float32);\n",
        "Y_train1 = np.squeeze(Y_train1.astype(np.int64))\n",
        "Y_test1 = np.squeeze(Y_test1.astype(np.int64))\n",
        "\n",
        "X_train2 = np.load(\"./ct2/X_train.npy\")\n",
        "Y_train2 = np.load(\"./ct2/Y_train.npy\")\n",
        "X_test2 = np.load(\"./ct2/X_test.npy\")\n",
        "Y_test2 = np.load(\"./ct2/Y_test.npy\")\n",
        "X_train2 = X_train2[:, np.newaxis].astype(np.float32);\n",
        "X_test2 = X_test2[:, np.newaxis].astype(np.float32);\n",
        "Y_train2 = np.squeeze(Y_train2.astype(np.int64))\n",
        "Y_test2 = np.squeeze(Y_test2.astype(np.int64))\n",
        "\n",
        "X_train3 = np.load(\"./ct3/X_train.npy\")\n",
        "Y_train3 = np.load(\"./ct3/Y_train.npy\")\n",
        "X_test3 = np.load(\"./ct3/X_test.npy\")\n",
        "Y_test3 = np.load(\"./ct3/Y_test.npy\")\n",
        "X_train3 = X_train3[:, np.newaxis].astype(np.float32);\n",
        "X_test3 = X_test3[:, np.newaxis].astype(np.float32);\n",
        "Y_train3 = np.squeeze(Y_train3.astype(np.int64))\n",
        "Y_test3 = np.squeeze(Y_test3.astype(np.int64))\n",
        "\n",
        "\n",
        "print(\"============Statistical Information===========\")\n",
        "print(\"Training Set: \")\n",
        "print(\"Shape of X_train0: \" + str(X_train0.shape) + \"; Shape of labels: \" + str(Y_train0.shape))\n",
        "print(\"Shape of X_train1: \" + str(X_train1.shape) + \"; Shape of labels: \" + str(Y_train1.shape))\n",
        "print(\"Shape of X_train2: \" + str(X_train2.shape) + \"; Shape of labels: \" + str(Y_train2.shape))\n",
        "print(\"Shape of X_train3: \" + str(X_train3.shape) + \"; Shape of labels: \" + str(Y_train3.shape))\n",
        "\n",
        "print(\"Test Set: \")\n",
        "print(\"Shape of X_test0: \" + str(X_test0.shape) + \"; Shape of labels: \" + str(Y_test0.shape))\n",
        "print(\"Shape of X_test1: \" + str(X_test1.shape) + \"; Shape of labels: \" + str(Y_test1.shape))\n",
        "print(\"Shape of X_test2: \" + str(X_test2.shape) + \"; Shape of labels: \" + str(Y_test2.shape))\n",
        "print(\"Shape of X_test3: \" + str(X_test3.shape) + \"; Shape of labels: \" + str(Y_test3.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtomhVk8wFWH",
        "outputId": "5b7d3a57-a55e-404a-d94d-7ce7cd052973",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2448, 1, 64, 64)\n",
            "(2448, 64, 64, 1)\n",
            "#### discriminator ######\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        1088      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 256)         524544    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16384)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 16385     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 673,217\n",
            "Trainable params: 673,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "#### generator ######\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 8192)              827392    \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 8192)              0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " up_sampling2d (UpSampling2D  (None, 16, 16, 128)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        131136    \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " up_sampling2d_1 (UpSampling  (None, 32, 32, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 32)        32800     \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " up_sampling2d_2 (UpSampling  (None, 64, 64, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 64, 64, 1)         513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 991,841\n",
            "Trainable params: 991,841\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [39:12<00:00,  5.88s/it]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from tqdm import tqdm\n",
        "import anogan\n",
        "\n",
        "X_trainmy = X_train0\n",
        "print(X_trainmy.shape)\n",
        "\n",
        "X_trainmy = X_trainmy.astype(np.float32)/255.\n",
        "X_trainmy = np.transpose(X_trainmy, (0, 2, 3, 1))\n",
        "print(X_trainmy.shape)\n",
        "\n",
        "Model_d, Model_g = anogan.train(32, X_trainmy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ezrJF_YcKUm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import anogan\n",
        "\n",
        "#extra train\n",
        "\n",
        "########################\n",
        "#X_trainmy = X_train\n",
        "#print(X_trainmy.shape)\n",
        "\n",
        "#X_trainmy = X_trainmy.astype(np.float32)/255.\n",
        "#X_trainmy = np.transpose(X_trainmy, (0, 2, 3, 1))\n",
        "#print(X_trainmy.shape)\n",
        "########################\n",
        "\n",
        "def extra_train(BATCH_SIZE, X_train):\n",
        "    d = anogan.discriminator_model()\n",
        "    d.load_weights('assets/discriminator')\n",
        "    g = anogan.generator_model()\n",
        "    g.load_weights('assets/generator')\n",
        "\n",
        "    d_on_g = anogan.generator_containing_discriminator(g, d)\n",
        "    d.trainable = True\n",
        "    for epoch in tqdm(range(100)):\n",
        "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
        "            noise = np.random.uniform(0, 1, size=(BATCH_SIZE, 100))\n",
        "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "            generated_images = g.predict(noise, verbose=0)\n",
        "            X = np.concatenate((image_batch, generated_images))\n",
        "            y = np.array([1] * BATCH_SIZE + [0] * BATCH_SIZE)\n",
        "            d_loss = d.train_on_batch(X, y)\n",
        "            noise = np.random.uniform(0, 1, (BATCH_SIZE, 100))\n",
        "            d.trainable = False\n",
        "            g_loss = d_on_g.train_on_batch(noise, np.array([1] * BATCH_SIZE))\n",
        "            d.trainable = True\n",
        "        g.save_weights('assets/generator', True)\n",
        "        d.save_weights('assets/discriminator', True)\n",
        "    return d, g\n",
        "\n",
        "extra_Model_d, extra_Model_g = extra_train(64, X_trainmy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InwGIc25wFWJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import anogan\n",
        "\n",
        "## generate random image \n",
        "\n",
        "generated_img = anogan.generate(4)\n",
        "\n",
        "for i in range(len(generated_img)):\n",
        "    plt.figure(figsize=(2, 2))\n",
        "    plt.imshow(generated_img[i].reshape(64, 64),cmap=plt.cm.gray)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghZH35SYM_9e"
      },
      "outputs": [],
      "source": [
        "!mkdir rst\n",
        "\n",
        "# Append-adds at last\n",
        "#file1 = open(\"./ano_score.txt\", \"a\")  # append mode\n",
        "\n",
        "myX_test = X_test0\n",
        "myX_test = myX_test.astype(np.float32)/255.\n",
        "myX_test = myX_test.reshape(-1, 64, 64, 1)\n",
        "#X_train = X_train0\n",
        "#X_train = X_train.astype(np.float32)/255.\n",
        "#X_train = X_train.reshape(-1, 64, 64, 1)\n",
        "y_test_label = 0\n",
        "\n",
        "model = anogan.anomaly_detector()\n",
        "\n",
        "for i in range(len(myX_test)):\n",
        "    test_img = myX_test[i]\n",
        "    #train_img = myX_train[i]\n",
        "    testname = \"./rst/\" + str(y_test_label) + \"_test\" + str(i) + \".png\"\n",
        "    trainname = \"./rst/train\" + str(i) + \".png\"\n",
        "    fakename = \"./rst/fake\" + str(i) + \".png\"\n",
        "    rstname = \"./rst/ano_test\" + str(i) + \".png\"\n",
        "    \n",
        "    ano_score, similar_img = anogan.compute_anomaly_score(model, test_img.reshape(1, 64, 64, 1))\n",
        "\n",
        "    #plt.figure(figsize=(2, 2))\n",
        "    #plt.imshow(train_img.reshape(64,64), cmap=plt.cm.gray)\n",
        "    #plt.show()\n",
        "    #plt.savefig(trainname)\n",
        "\n",
        "    plt.figure(figsize=(2, 2))\n",
        "    plt.imshow(test_img.reshape(64,64), cmap=plt.cm.gray)\n",
        "    plt.show()\n",
        "    #plt.savefig(testname)\n",
        "    print(\"anomaly score : \" + str(ano_score) + \", label :\" + str(y_test_label) )\n",
        "    #file1.write(\"anomaly score : \" + str(ano_score) + \", label :\" + str(y_test_label) + \"\\n\")\n",
        "    \n",
        "    plt.figure(figsize=(2, 2))\n",
        "    plt.imshow(similar_img.reshape(64,64), cmap=plt.cm.gray)\n",
        "    plt.show()\n",
        "    #plt.savefig(fakename)\n",
        "\n",
        "    plt.figure(figsize=(2, 2))\n",
        "    plt.imshow(test_img.reshape(64,64), cmap=plt.cm.gray)\n",
        "    residual  = test_img.reshape(64,64) - similar_img.reshape(64, 64)\n",
        "    plt.imshow(residual, cmap='jet', alpha=.5)\n",
        "    plt.show()\n",
        "    #plt.savefig(rstname)\n",
        "\n",
        "#file1.close()    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "anoGAN64x64_covid.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}