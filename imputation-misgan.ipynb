{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MisGAN: Learning from Incomplete Data with GANs\n",
    "\n",
    "This notebook includes the source code for missing data imputation using MisGAN on MosMedData with CT scans.\n",
    "\n",
    "We first import necessary libraries and hardward configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "import pylab as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print('Device: %s' % device)\n",
    "\n",
    "input_size = 128\n",
    "print('Input Size: %d x %d' % (input_size, input_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incomplete MosMed dataset\n",
    "\n",
    "Then, we create the incomplete MosMed dataset for training the MisGAN. We introduce a simulation process by removing part of the observation in CT images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockMaskedMosMed(Dataset):\n",
    "    def __init__(self, block_len, data_dir=None, random_seed=0):\n",
    "        self.block_len = block_len\n",
    "        self.rnd = np.random.RandomState(random_seed)\n",
    "        \n",
    "        data_dir='./data_%dx%d_345_.npy' % (input_size, input_size)\n",
    "        d = np.load(data_dir)[:, np.newaxis, :, :]\n",
    "        data = TensorDataset(torch.Tensor(d))\n",
    "        \n",
    "        self.data_size = len(data)\n",
    "        self.generate_incomplete_data(data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return index so we can retrieve the mask location from self.mask_loc\n",
    "        return self.image[index], self.mask[index], index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def generate_incomplete_data(self, data):\n",
    "        n_masks = self.data_size\n",
    "        self.image = [None] * n_masks\n",
    "        self.mask = [None] * n_masks\n",
    "        self.mask_loc = [None] * n_masks\n",
    "        for i in range(n_masks):\n",
    "            d0 = self.rnd.randint(0, input_size - self.block_len + 1)\n",
    "            d1 = self.rnd.randint(0, input_size - self.block_len + 1)\n",
    "            mask = torch.ones((input_size, input_size), dtype=torch.uint8)\n",
    "            mask[d0:(d0 + self.block_len), d1:(d1 + self.block_len)] = 0\n",
    "            self.mask[i] = mask.unsqueeze(0)   # add an axis for channel\n",
    "            self.mask_loc[i] = d0, d1, self.block_len, self.block_len\n",
    "            # Mask out missing pixels by zero\n",
    "            self.image[i] = data[i][0] * mask.float()\n",
    "\n",
    "data = BlockMaskedMosMed(block_len=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a modified MosMed dataset using `BlockMaskedMosMed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_loader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implement `plot_grid()` for plotting input images on a grid\n",
    "of `nrow` rows and `ncol` columns.\n",
    "An optional argument `bbox` can be provided as a list of (x, y, width, height)\n",
    "to draw a red rectangular frame with that coordinate on each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(ax, image, bbox=None, gap=1, gap_value=1, nrow=4, ncol=8, title=None):\n",
    "    image = image.cpu().numpy().squeeze(1)\n",
    "    LEN = input_size\n",
    "    grid = np.empty((nrow * (LEN + gap) - gap, ncol * (LEN + gap) - gap))\n",
    "    grid.fill(gap_value)\n",
    "\n",
    "    for i, x in enumerate(image):\n",
    "        if i >= nrow * ncol:\n",
    "            break\n",
    "        p0 = (i // ncol) * (LEN + gap)\n",
    "        p1 = (i % ncol) * (LEN + gap)\n",
    "        grid[p0:(p0 + LEN), p1:(p1 + LEN)] = x\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(grid, interpolation='none', aspect='equal')\n",
    "\n",
    "    if bbox:\n",
    "        nplot = min(len(image), nrow * ncol)\n",
    "        for i in range(nplot):\n",
    "            d0, d1, d0_len, d1_len = bbox[i]\n",
    "            p0 = (i // ncol) * (LEN + gap)\n",
    "            p1 = (i % ncol) * (LEN + gap)\n",
    "            offset = np.array([p1 + d1, p0 + d0]) - .5\n",
    "            ax.add_patch(Rectangle(\n",
    "                offset, d1_len, d0_len, lw=1.5, edgecolor='red', fill=False))\n",
    "            \n",
    "    if title:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking operator\n",
    "\n",
    "Here we implement the masking operator\n",
    "$f_\\tau(\\mathbf{x}, \\mathbf{m}) = \\mathbf{x} \\odot \\mathbf{m} + \\tau\\bar{\\mathbf{m}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_data(data, mask, tau=0):\n",
    "\n",
    "    return mask * data + (1 - mask) * tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of incomplete MosMed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples, mask_samples, _ = next(iter(data_loader))\n",
    "fig, ax = plt.subplots(figsize=(17, 5))\n",
    "plot_grid(ax, mask_data(data_samples, mask_samples.float(), data_samples),\n",
    "          nrow=4, ncol=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MisGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the generator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must sub-class ConvGenerator to provide transform()\n",
    "class ConvGenerator(nn.Module):\n",
    "    def __init__(self, latent_size=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.DIM = 64\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.preprocess = nn.Sequential(\n",
    "            nn.Linear(latent_size, 4 * 4 * 4 * 4 * 4 * self.DIM),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4 * self.DIM, 3 * self.DIM, 5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(3 * self.DIM, 2 * self.DIM, 5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * self.DIM, self.DIM, 6),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1 * self.DIM, self.DIM // 2, 5, stride=2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.deconv_out = nn.ConvTranspose2d(self.DIM // 2, 1, 8, stride=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        net = self.preprocess(input)\n",
    "        net = net.view(-1, 4 * self.DIM, 16, 16)\n",
    "        net = self.block1(net)\n",
    "        net = self.block2(net)\n",
    "        net = self.block3(net)\n",
    "        net = self.block4(net)\n",
    "        net = self.deconv_out(net)\n",
    "        return self.transform(net).view(-1, 1, input_size, input_size)\n",
    "\n",
    "\n",
    "class ConvDataGenerator(ConvGenerator):\n",
    "    def __init__(self, latent_size=128):\n",
    "        super().__init__(latent_size=latent_size)\n",
    "        self.transform = lambda x: torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class ConvMaskGenerator(ConvGenerator):\n",
    "    def __init__(self, latent_size=128, temperature=.66):\n",
    "        super().__init__(latent_size=latent_size)\n",
    "        self.transform = lambda x: torch.sigmoid(x / temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the discriminator class which is also called critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.DIM = 64\n",
    "        main = nn.Sequential(\n",
    "            nn.Conv2d(1, self.DIM, 5, stride=2, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.DIM, 2 * self.DIM, 5, stride=2, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(2 * self.DIM, 4 * self.DIM, 5, stride=2, padding=2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.main = main\n",
    "        self.output = nn.Linear(4 * 4 * 4 * 4 * 4 * self.DIM, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, 1, input_size, input_size)\n",
    "        net = self.main(input)\n",
    "        net = net.view(-1, 4 * 4 * 4 * 4 * 4 * self.DIM)\n",
    "        net = self.output(net)\n",
    "        return net.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the updater using Wasserstein GAN with gradient penality for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticUpdater:\n",
    "    def __init__(self, critic, critic_optimizer, batch_size=64, gp_lambda=10):\n",
    "        self.critic = critic\n",
    "        self.critic_optimizer = critic_optimizer\n",
    "        self.gp_lambda = gp_lambda\n",
    "        # Interpolation coefficient\n",
    "        self.eps = torch.empty(batch_size, 1, 1, 1, device=device)\n",
    "        # For computing the gradient penalty\n",
    "        self.ones = torch.ones(batch_size).to(device)\n",
    "\n",
    "    def __call__(self, real, fake):\n",
    "        real = real.detach()\n",
    "        fake = fake.detach()\n",
    "        self.critic.zero_grad()\n",
    "        self.eps.uniform_(0, 1)\n",
    "        interp = (self.eps * real + (1 - self.eps) * fake).requires_grad_()\n",
    "        grad_d = grad(self.critic(interp), interp, grad_outputs=self.ones,\n",
    "                      create_graph=True)[0]\n",
    "        grad_d = grad_d.view(real.shape[0], -1)\n",
    "        grad_penalty = ((grad_d.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
    "        w_dist = self.critic(fake).mean() - self.critic(real).mean()\n",
    "        loss = w_dist + grad_penalty\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate all the building blocks for MisGAN: the data/mask\n",
    "generators and their corresponding discriminators.\n",
    "We use the [Adam optimizer](https://arxiv.org/abs/1412.6980) to train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 128   # dimensionality of the latent code\n",
    "n_critic = 5\n",
    "alpha = .2\n",
    "\n",
    "data_gen = ConvDataGenerator().to(device)\n",
    "mask_gen = ConvMaskGenerator().to(device)\n",
    "\n",
    "data_critic = ConvCritic().to(device)\n",
    "mask_critic = ConvCritic().to(device)\n",
    "\n",
    "data_noise = torch.empty(batch_size, nz, device=device)\n",
    "mask_noise = torch.empty(batch_size, nz, device=device)\n",
    "\n",
    "lrate = 1e-4\n",
    "data_gen_optimizer = optim.Adam(data_gen.parameters(), lr=lrate, betas=(.5, .9))\n",
    "mask_gen_optimizer = optim.Adam(mask_gen.parameters(), lr=lrate, betas=(.5, .9))\n",
    "\n",
    "data_critic_optimizer = optim.Adam(data_critic.parameters(), lr=lrate, betas=(.5, .9))\n",
    "mask_critic_optimizer = optim.Adam(mask_critic.parameters(), lr=lrate, betas=(.5, .9))\n",
    "\n",
    "update_data_critic = CriticUpdater(data_critic, data_critic_optimizer, batch_size)\n",
    "update_mask_critic = CriticUpdater(mask_critic, mask_critic_optimizer, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin the training for 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interval = 10\n",
    "critic_updates = 0\n",
    "\n",
    "for epoch in tqdm(range(500)):\n",
    "    for real_data, real_mask, _ in data_loader:\n",
    "\n",
    "        real_data = real_data.to(device)\n",
    "        real_mask = real_mask.to(device).float()\n",
    "\n",
    "        # Update discriminators' parameters\n",
    "        data_noise.normal_()\n",
    "        mask_noise.normal_()\n",
    "\n",
    "        fake_data = data_gen(data_noise)\n",
    "        fake_mask = mask_gen(mask_noise)\n",
    "\n",
    "        masked_fake_data = mask_data(fake_data, fake_mask)\n",
    "        masked_real_data = mask_data(real_data, real_mask)\n",
    "\n",
    "        update_data_critic(masked_real_data, masked_fake_data)\n",
    "        update_mask_critic(real_mask, fake_mask)\n",
    "\n",
    "        critic_updates += 1\n",
    "\n",
    "        if critic_updates == n_critic:\n",
    "            critic_updates = 0\n",
    "\n",
    "            # Update generators' parameters\n",
    "            for p in data_critic.parameters():\n",
    "                p.requires_grad_(False)\n",
    "            for p in mask_critic.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "            data_gen.zero_grad()\n",
    "            mask_gen.zero_grad()\n",
    "\n",
    "            data_noise.normal_()\n",
    "            mask_noise.normal_()\n",
    "\n",
    "            fake_data = data_gen(data_noise)\n",
    "            fake_mask = mask_gen(mask_noise)\n",
    "            masked_fake_data = mask_data(fake_data, fake_mask)\n",
    "\n",
    "            data_loss = -data_critic(masked_fake_data).mean()\n",
    "            data_loss.backward(retain_graph=True)\n",
    "            data_gen_optimizer.step()\n",
    "\n",
    "            mask_loss = -mask_critic(fake_mask).mean()\n",
    "            (mask_loss + data_loss * alpha).backward()\n",
    "            mask_gen_optimizer.step()\n",
    "\n",
    "            for p in data_critic.parameters():\n",
    "                p.requires_grad_(True)\n",
    "            for p in mask_critic.parameters():\n",
    "                p.requires_grad_(True)\n",
    "\n",
    "    if plot_interval > 0 and (epoch + 1) % plot_interval == 0:\n",
    "        # Although it makes no difference setting eval() in this example, \n",
    "        # you will need those if you are going to use modules such as \n",
    "        # batch normalization or dropout in the generators.\n",
    "        data_gen.eval()\n",
    "        mask_gen.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print('Epoch:', epoch)\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 4.5))\n",
    "            \n",
    "            data_noise.normal_()\n",
    "            data_samples = data_gen(data_noise)\n",
    "            plot_grid(ax1, data_samples, title='generated complete data')\n",
    "            \n",
    "            mask_noise.normal_()\n",
    "            mask_samples = mask_gen(mask_noise)\n",
    "            plot_grid(ax2, mask_samples, title='generated masks')\n",
    "            \n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "\n",
    "        data_gen.train()\n",
    "        mask_gen.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first phase of training, we save the trained models as checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data_gen.state_dict(), './misgan_128_square_missing_datagen_stage1.pt')\n",
    "torch.save(mask_gen.state_dict(), './misgan_128_square_missing_maskgen_stage1.pt')\n",
    "torch.save(data_critic.state_dict(), './misgan_128_square_missing_datacritic_stage1.pt')\n",
    "torch.save(mask_critic.state_dict(), './misgan_128_square_missing_maskcritic_stage1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second phase, we create an imputer class for synthesizing missing details on CT images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imputer(nn.Module):\n",
    "    def __init__(self, arch=(1024, 512)):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size * input_size, arch[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(arch[0], arch[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(arch[1], arch[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(arch[1], arch[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(arch[0], input_size * input_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, data, mask, noise):\n",
    "        net = data * mask + noise * (1 - mask)\n",
    "        net = net.view(data.shape[0], -1)\n",
    "        net = self.fc(net)\n",
    "        net = torch.sigmoid(net).view(data.shape)\n",
    "        return data * mask + net * (1 - mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate the imputer and the corresponding discriminator.\n",
    "We also use the Adam optimizer to train them.\n",
    "Note that for MisGAN imputation, we will re-use most of the components\n",
    "created earlier for MisGAN including the data/mask generators and\n",
    "the discriminators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer().to(device)\n",
    "impu_critic = ConvCritic().to(device)\n",
    "impu_noise = torch.empty(batch_size, 1, input_size, input_size, device=device)\n",
    "\n",
    "imputer_lrate = 2e-4\n",
    "imputer_optimizer = optim.Adam(\n",
    "    imputer.parameters(), lr=imputer_lrate, betas=(.5, .9))\n",
    "impu_critic_optimizer = optim.Adam(\n",
    "    impu_critic.parameters(), lr=imputer_lrate, betas=(.5, .9))\n",
    "update_impu_critic = CriticUpdater(\n",
    "    impu_critic, impu_critic_optimizer, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then begin the training for 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = .1\n",
    "plot_interval = 10\n",
    "critic_updates = 0\n",
    "\n",
    "for epoch in tqdm(range(500)):\n",
    "    for real_data, real_mask, index in data_loader:\n",
    "\n",
    "        real_data = real_data.to(device)\n",
    "        real_mask = real_mask.to(device).float()\n",
    "\n",
    "        masked_real_data = mask_data(real_data, real_mask)\n",
    "\n",
    "        # Update discriminators' parameters\n",
    "        data_noise.normal_()\n",
    "        fake_data = data_gen(data_noise)\n",
    "\n",
    "        mask_noise.normal_()\n",
    "        fake_mask = mask_gen(mask_noise)\n",
    "        masked_fake_data = mask_data(fake_data, fake_mask)\n",
    "\n",
    "        impu_noise.uniform_()\n",
    "        imputed_data = imputer(real_data, real_mask, impu_noise)\n",
    "\n",
    "        update_data_critic(masked_real_data, masked_fake_data)\n",
    "        update_mask_critic(real_mask, fake_mask)\n",
    "        update_impu_critic(fake_data, imputed_data)\n",
    "\n",
    "        critic_updates += 1\n",
    "\n",
    "        if critic_updates == n_critic:\n",
    "            critic_updates = 0\n",
    "\n",
    "            # Update generators' parameters\n",
    "            for p in data_critic.parameters():\n",
    "                p.requires_grad_(False)\n",
    "            for p in mask_critic.parameters():\n",
    "                p.requires_grad_(False)\n",
    "            for p in impu_critic.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "            data_noise.normal_()\n",
    "            fake_data = data_gen(data_noise)\n",
    "\n",
    "            mask_noise.normal_()\n",
    "            fake_mask = mask_gen(mask_noise)\n",
    "            masked_fake_data = mask_data(fake_data, fake_mask)\n",
    "\n",
    "            impu_noise.uniform_()\n",
    "            imputed_data = imputer(real_data, real_mask, impu_noise)\n",
    "\n",
    "            data_loss = -data_critic(masked_fake_data).mean()\n",
    "            mask_loss = -mask_critic(fake_mask).mean()\n",
    "            impu_loss = -impu_critic(imputed_data).mean()\n",
    "\n",
    "            mask_gen.zero_grad()\n",
    "            (mask_loss + data_loss * alpha).backward(retain_graph=True)\n",
    "            mask_gen_optimizer.step()\n",
    "\n",
    "            data_gen.zero_grad()\n",
    "            (data_loss + impu_loss * beta).backward(retain_graph=True)\n",
    "            data_gen_optimizer.step()\n",
    "\n",
    "            imputer.zero_grad()\n",
    "            impu_loss.backward()\n",
    "            imputer_optimizer.step()\n",
    "\n",
    "            for p in data_critic.parameters():\n",
    "                p.requires_grad_(True)\n",
    "            for p in mask_critic.parameters():\n",
    "                p.requires_grad_(True)\n",
    "            for p in impu_critic.parameters():\n",
    "                p.requires_grad_(True)\n",
    "\n",
    "    if plot_interval > 0 and (epoch + 1) % plot_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            imputer.eval()\n",
    "\n",
    "            # Plot imputation results\n",
    "            impu_noise.uniform_()\n",
    "            imputed_data = imputer(real_data, real_mask, impu_noise)\n",
    "            bbox = [data.mask_loc[idx] for idx in index]\n",
    "            print('Epoch:', epoch)\n",
    "            fig, ax = plt.subplots(figsize=(17, 8))\n",
    "            plot_grid(ax, imputed_data, bbox, gap=2)\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "\n",
    "            imputer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the trained models are saved for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data_gen.state_dict(), './misgan_128_square_missing_datagen_stage2.pt')\n",
    "torch.save(mask_gen.state_dict(), './misgan_128_square_missing_maskgen_stage2.pt')\n",
    "torch.save(data_critic.state_dict(), './misgan_128_square_missing_datacritic_stage2.pt')\n",
    "torch.save(mask_critic.state_dict(), './misgan_128_square_missing_maskcritic_stage2.pt')\n",
    "torch.save(imputer.state_dict(), './misgan_128_square_missing_imputer_stage2.pt')\n",
    "torch.save(impu_critic.state_dict(), './misgan_128_square_missing_imputercritic_stage2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}